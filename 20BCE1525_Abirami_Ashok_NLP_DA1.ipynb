{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**20BCE1525 Abirami Unna Ashok - NLP DA1**"
      ],
      "metadata": {
        "id": "YQlfQcy-d-zO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"all\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-y042asp3K8",
        "outputId": "f3e0cd4a-e4d8-4fe2-e2ae-eef880bdc988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1**"
      ],
      "metadata": {
        "id": "6mphieYjd7gE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "import numpy as np\n",
        "print(\"Categories:\", brown.categories())\n",
        "print(\"Number of categories:\", np.size(brown.categories()))\n",
        "print(\"Words:\", brown.words())\n",
        "print(\"Number of words:\", np.size(brown.words()))\n",
        "print(\"Words in category mystery:\", brown.words(categories = 'mystery'))\n",
        "print(\"Number of words in category mystery:\", np.size(brown.words(categories = 'mystery')))\n",
        "print(\"Words in category government:\", brown.words(categories = 'government'))\n",
        "print(\"Number of words in category government:\", np.size(brown.words(categories = 'government')))\n",
        "print(\"Sentences:\", brown.sents())\n",
        "print(\"Number of sentences:\", np.size(brown.sents()))\n",
        "print(\"Sentences in category mystery:\", brown.sents(categories = 'mystery'))\n",
        "print(\"Number of sentences in category mystery:\", np.size(brown.sents(categories = 'mystery')))\n",
        "print(\"Paragraphs in category mystery:\", brown.paras(categories = 'mystery'))\n",
        "print(\"Number of paragraphs in category mystery:\", np.size(brown.paras(categories = 'mystery')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF0R0Rq-qU9P",
        "outputId": "769c8f58-51c4-464e-8c36-44494662bd71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categories: ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
            "Number of categories: 15\n",
            "Words: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
            "Number of words: 1161192\n",
            "Words in category mystery: ['There', 'were', 'thirty-eight', 'patients', 'on', ...]\n",
            "Number of words in category mystery: 57169\n",
            "Words in category government: ['The', 'Office', 'of', 'Business', 'Economics', '(', ...]\n",
            "Number of words in category government: 70117\n",
            "Sentences: [['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]\n",
            "Number of sentences: 57340\n",
            "Sentences in category mystery: [['There', 'were', 'thirty-eight', 'patients', 'on', 'the', 'bus', 'the', 'morning', 'I', 'left', 'for', 'Hanover', ',', 'most', 'of', 'them', 'disturbed', 'and', 'hallucinating', '.'], ['An', 'interne', ',', 'a', 'nurse', 'and', 'two', 'attendants', 'were', 'in', 'charge', 'of', 'us', '.'], ...]\n",
            "Number of sentences in category mystery: 3886\n",
            "Paragraphs in category mystery: [[['There', 'were', 'thirty-eight', 'patients', 'on', 'the', 'bus', 'the', 'morning', 'I', 'left', 'for', 'Hanover', ',', 'most', 'of', 'them', 'disturbed', 'and', 'hallucinating', '.'], ['An', 'interne', ',', 'a', 'nurse', 'and', 'two', 'attendants', 'were', 'in', 'charge', 'of', 'us', '.']], [['I', 'felt', 'lonely', 'and', 'depressed', 'as', 'I', 'stared', 'out', 'the', 'bus', 'window', 'at', \"Chicago's\", 'grim', ',', 'dirty', 'West', 'Side', '.'], ['It', 'seemed', 'incredible', ',', 'as', 'I', 'listened', 'to', 'the', 'monotonous', 'drone', 'of', 'voices', 'and', 'smelled', 'the', 'fetid', 'odors', 'coming', 'from', 'the', 'patients', ',', 'that', 'technically', 'I', 'was', 'a', 'ward', 'of', 'the', 'state', 'of', 'Illinois', ',', 'going', 'to', 'a', 'hospital', 'for', 'the', 'mentally', 'ill', '.']], ...]\n",
            "Number of paragraphs in category mystery: 1164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "newstext = brown.words(categories = 'news')\n",
        "fdist = FreqDist(w.lower() for w in newstext)\n",
        "print(fdist)\n",
        "print(\"Frequency distribution:\")\n",
        "fdist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWsjUKHW5btd",
        "outputId": "2f432304-f58e-412a-ec6d-de865eb701d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<FreqDist with 13112 samples and 100554 outcomes>\n",
            "Frequency distribution:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'the': 6386, ',': 5188, '.': 4030, 'of': 2861, 'and': 2186, 'to': 2144, 'a': 2130, 'in': 2020, 'for': 969, 'that': 829, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total number of tokens:\", fdist.N())\n",
        "print(\"Number of times token 'and' appears:\", fdist['and'])\n",
        "print(\"Number of times token 'news' appears:\", fdist['news'])\n",
        "print(\"Number of times token 'in' appears:\", fdist['in'])\n",
        "print(\"Most frequent tokens are: 'the' which appears\", fdist['the'], \"times, 'of' which appears\", fdist['of'], \"times and 'and' which appears\", fdist['and'], \"times.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvP-qqDS6AyL",
        "outputId": "b676b31a-7833-4a58-94d5-cfd35afc8be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tokens: 100554\n",
            "Number of times token 'and' appears: 2186\n",
            "Number of times token 'news' appears: 14\n",
            "Number of times token 'in' appears: 2020\n",
            "Most frequent tokens are: 'the' which appears 6386 times, 'of' which appears 2861 times and 'and' which appears 2186 times.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"At Sprinkles, we bake and sell a variety of cakes, cupcakes and cookies. Each order is made freshly and with the finest of ingredients. The secret ingredient here is love! There are many details to be processed during the time of ordering, like the quantity, size, flavour, decorations, design, date and time of delivery and so on.\"\n",
        "from nltk.tokenize import word_tokenize\n",
        "print(\"Word tokens:\", word_tokenize(text))\n",
        "print(\"Number of word tokens using word_tokenize:\", np.size(word_tokenize(text)))\n",
        "from nltk.tokenize.regexp import WordPunctTokenizer\n",
        "toks = WordPunctTokenizer().tokenize(text)\n",
        "print(\"Number of tokens using WordPunctTokenizer:\", np.size(toks))\n",
        "from nltk.tokenize.regexp import WhitespaceTokenizer\n",
        "tokens = WhitespaceTokenizer().tokenize(text)\n",
        "print(\"Number of tokens using WhitespaceTokenizer:\", np.size(tokens))\n",
        "from nltk.tokenize import sent_tokenize\n",
        "print(\"Sentence tokens:\", sent_tokenize(text))\n",
        "print(\"Number of sentence tokens:\", np.size(sent_tokenize(text)))\n",
        "words = nltk.word_tokenize(text)\n",
        "unique_tokens = set(words)\n",
        "print(\"Number of unique tokens:\", len(unique_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMtcEbWZ1HiM",
        "outputId": "a7ce5225-b986-4f46-a550-50c9b70ff412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokens: ['At', 'Sprinkles', ',', 'we', 'bake', 'and', 'sell', 'a', 'variety', 'of', 'cakes', ',', 'cupcakes', 'and', 'cookies', '.', 'Each', 'order', 'is', 'made', 'freshly', 'and', 'with', 'the', 'finest', 'of', 'ingredients', '.', 'The', 'secret', 'ingredient', 'here', 'is', 'love', '!', 'There', 'are', 'many', 'details', 'to', 'be', 'processed', 'during', 'the', 'time', 'of', 'ordering', ',', 'like', 'the', 'quantity', ',', 'size', ',', 'flavour', ',', 'decorations', ',', 'design', ',', 'date', 'and', 'time', 'of', 'delivery', 'and', 'so', 'on', '.']\n",
            "Number of word tokens using word_tokenize: 69\n",
            "Number of tokens using WordPunctTokenizer: 69\n",
            "Number of tokens using WhitespaceTokenizer: 57\n",
            "Sentence tokens: ['At Sprinkles, we bake and sell a variety of cakes, cupcakes and cookies.', 'Each order is made freshly and with the finest of ingredients.', 'The secret ingredient here is love!', 'There are many details to be processed during the time of ordering, like the quantity, size, flavour, decorations, design, date and time of delivery and so on.']\n",
            "Number of sentence tokens: 4\n",
            "Number of unique tokens: 49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(text)\n",
        "fdist = FreqDist(words)\n",
        "filtered_word_freq = dict((word, freq) for word, freq in fdist.items() if not word.isdigit())\n",
        "print(\"Number of times each token appears:\")\n",
        "print(filtered_word_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXJly47ADvDu",
        "outputId": "70e2023e-dca0-4518-8ffd-62c9527e2269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of times each token appears:\n",
            "{'At': 1, 'Sprinkles': 1, ',': 8, 'we': 1, 'bake': 1, 'and': 5, 'sell': 1, 'a': 1, 'variety': 1, 'of': 4, 'cakes': 1, 'cupcakes': 1, 'cookies': 1, '.': 3, 'Each': 1, 'order': 1, 'is': 2, 'made': 1, 'freshly': 1, 'with': 1, 'the': 3, 'finest': 1, 'ingredients': 1, 'The': 1, 'secret': 1, 'ingredient': 1, 'here': 1, 'love': 1, '!': 1, 'There': 1, 'are': 1, 'many': 1, 'details': 1, 'to': 1, 'be': 1, 'processed': 1, 'during': 1, 'time': 2, 'ordering': 1, 'like': 1, 'quantity': 1, 'size': 1, 'flavour': 1, 'decorations': 1, 'design': 1, 'date': 1, 'delivery': 1, 'so': 1, 'on': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2**"
      ],
      "metadata": {
        "id": "OKSdZQwhefum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Raw corpus\n",
        "import nltk\n",
        "f = open('doc.txt')\n",
        "for line in f:\n",
        " print(line.strip())\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwsGJ7tGelw_",
        "outputId": "aad37e51-1126-4ada-cd8d-323728a77d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At Sprinkles, we bake and sell a variety of cakes, cupcakes and cookies. Each order is made freshly and with the finest of ingredients. The secret ingredient here is love! There are many details to be processed during the time of ordering, like the quantity, size, flavour, decorations, design, date and time of delivery and so on.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('doc.txt')\n",
        "raw = f.read()\n",
        "raw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "Fwxmkoq4xTZO",
        "outputId": "4d8f8dca-bab4-4311-f7d1-c040a8438d67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'At Sprinkles, we bake and sell a variety of cakes, cupcakes and cookies. Each order is made freshly and with the finest of ingredients. The secret ingredient here is love! There are many details to be processed during the time of ordering, like the quantity, size, flavour, decorations, design, date and time of delivery and so on.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib import request\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf8')\n",
        "print(\"Type of raw:\", type(raw))\n",
        "print(\"Length of raw:\", len(raw))\n",
        "print(\"'Part 1' starts at:\", raw.find(\"PART I\"))\n",
        "print(\"The word 'celebrity' is found at:\", raw.find(\"celebrity\"))\n",
        "print(\"Raw text:\", raw[:500], \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLQ11hBqlmxf",
        "outputId": "a02815d2-dc73-4f31-c208-97627f69dc6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of raw: <class 'str'>\n",
            "Length of raw: 1176812\n",
            "'Part 1' starts at: 5575\n",
            "The word 'celebrity' is found at: 1750\n",
            "Raw text: ﻿The Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere in the United States and\r\n",
            "most other parts of the world at no cost and with almost no restrictions\r\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\r\n",
            "of the Project Gutenberg License included with this eBook or online at\r\n",
            "www.gutenberg.org. If you are not located in the United States, you\r\n",
            "will have to check the laws of the country where you are located \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "tokens = word_tokenize(raw)\n",
        "print(\"Type of tokens:\", type(tokens))\n",
        "print(\"Length of tokens:\", len(tokens))\n",
        "print(\"Token text:\", tokens[1:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-Hl5CHMm59q",
        "outputId": "a6d20a34-99d1-4920-d522-9b94c2aa4a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of tokens: <class 'list'>\n",
            "Length of tokens: 257058\n",
            "Token text: ['Project', 'Gutenberg', 'eBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by', 'Fyodor', 'Dostoevsky', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.', 'You', 'may', 'copy', 'it', ',', 'give', 'it', 'away']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = nltk.Text(tokens)\n",
        "print(\"Type of text:\", type(text2))\n",
        "print(\"Collocations:\")\n",
        "text2.collocations()\n",
        "print(\"Text:\", text2[1200:1250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdUufe6uqKLQ",
        "outputId": "3efbaae4-87af-4118-af71-a54f96a9971c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of text: <class 'nltk.text.Text'>\n",
            "Collocations:\n",
            "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
            "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
            "woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\n",
            "great deal; young man; Nikodim Fomitch; Project Gutenberg; Ilya\n",
            "Petrovitch; Andrey Semyonovitch; Hay Market; Dmitri Prokofitch; Good\n",
            "heavens\n",
            "Text: ['frightened', 'feeling', ',', 'which', 'made', 'him', 'scowl', 'and', 'feel', 'ashamed', '.', 'He', 'was', 'hopelessly', 'in', 'debt', 'to', 'his', 'landlady', ',', 'and', 'was', 'afraid', 'of', 'meeting', 'her', '.', 'This', 'was', 'not', 'because', 'he', 'was', 'cowardly', 'and', 'abject', ',', 'quite', 'the', 'contrary', ';', 'but', 'for', 'some', 'time', 'past', 'he', 'had', 'been', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#POS tagged corpora\n",
        "import nltk\n",
        "text3 = word_tokenize(\"This is the third text being used.\")\n",
        "print(nltk.pos_tag(text3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MnbaTNfuE5s",
        "outputId": "12f62e8c-4b73-41fb-a0b2-3e00bf35cd2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('This', 'DT'), ('is', 'VBZ'), ('the', 'DT'), ('third', 'JJ'), ('text', 'NN'), ('being', 'VBG'), ('used', 'VBN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text4 = \"How are you? See you later. Take care.\"\n",
        "sentence = nltk.sent_tokenize(text4)\n",
        "for sent in sentence:\n",
        "\t print(nltk.pos_tag(nltk.word_tokenize(sent)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRdOMqasunkm",
        "outputId": "1c96e659-3893-4e65-88f7-d225ffd8355e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('How', 'WRB'), ('are', 'VBP'), ('you', 'PRP'), ('?', '.')]\n",
            "[('See', 'VB'), ('you', 'PRP'), ('later', 'RB'), ('.', '.')]\n",
            "[('Take', 'NNP'), ('care', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text5 = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
        "print(\"Text similar to 'woman':\")\n",
        "text5.similar('woman')\n",
        "print(\"\\n\")\n",
        "print(\"Text similar to 'in':\")\n",
        "text5.similar('in')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2TPrbsKw8Io",
        "outputId": "9bfa29d1-59f6-4568-a82b-2e8e5ba8092a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text similar to 'woman':\n",
            "man time day year car moment world house family child country boy\n",
            "state job place way war girl work word\n",
            "\n",
            "\n",
            "Text similar to 'in':\n",
            "of on and to for with at from by as that is was but when into the over\n",
            "if all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_token = nltk.tag.str2tuple('See/VB')\n",
        "print(tagged_token)\n",
        "print(tagged_token[0])\n",
        "print(tagged_token[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVJ7AEc-ykKb",
        "outputId": "5fbcc8cc-e34b-45de-f9a4-e21de705512a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('See', 'VB')\n",
            "See\n",
            "VB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(nltk.corpus.brown.tagged_words())\n",
        "print(nltk.corpus.brown.tagged_words(tagset = 'universal'))\n",
        "print(nltk.corpus.treebank.tagged_words())\n",
        "print(nltk.corpus.treebank.tagged_words(tagset = 'universal'))\n",
        "print(nltk.corpus.indian.tagged_words())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDLkiavPzGCe",
        "outputId": "a6d19b04-c5c8-40ec-856f-8e813d9328f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n",
            "[('The', 'DET'), ('Fulton', 'NOUN'), ...]\n",
            "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ...]\n",
            "[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ...]\n",
            "[('মহিষের', 'NN'), ('সন্তান', 'NN'), (':', 'SYM'), ...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "brown_news_tagged = brown.tagged_words(categories = 'news', tagset = 'universal')\n",
        "tag_fd = nltk.FreqDist(tag for (word, tag) in brown_news_tagged)\n",
        "print(tag_fd.most_common())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkeRgwVr1Erg",
        "outputId": "4d1510ee-5344-4c32-a7d4-9f192db7af23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NOUN', 30654), ('VERB', 14399), ('ADP', 12355), ('.', 11928), ('DET', 11389), ('ADJ', 6706), ('ADV', 3349), ('CONJ', 2717), ('PRON', 2535), ('PRT', 2264), ('NUM', 2166), ('X', 92)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3**"
      ],
      "metadata": {
        "id": "57P-98ZC1lkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "corpus_root = '/content/textfiles'\n",
        "corpus = PlaintextCorpusReader(corpus_root, '.*')"
      ],
      "metadata": {
        "id": "hFJ9M8p-3z9E"
      },
      "execution_count": 392,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Content of the corpus:\\n\", corpus.raw())\n",
        "print(\"\\n\")\n",
        "print(\"Content of the last two files:\\n\", corpus.raw(fileids=['file2.txt','file3.txt']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxHev9XPPnw1",
        "outputId": "8afcb944-a2a7-4a04-fbb6-b213555e94bb"
      },
      "execution_count": 393,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content of the corpus:\n",
            " The red glint of paint sparkled under the sun. He had dreamed of owning this car since he was ten, and that dream had become a reality less than a year ago. It was his baby and he spent hours caring for it, pampering it, and fondling over it.\r\n",
            "She knew this all too well, and that's exactly why she had taken a sledge hammer to it. She was in a hurry. Not the standard hurry when you're in a rush to get someplace, but a frantic hurry. The type of hurry where a few seconds could mean life or death. She raced down the road ignoring speed limits and weaving between cars. She was only a few minutes away when traffic came to a dead standstill on the road ahead.Here's the thing. She doesn't have anything to prove, but she is going to anyway. That's just her character.\r\n",
            "She knows she doesn't have to, but she still will just to show you that she can. Doubt her more and she'll prove she can again. We all already know this and you will too.\r\n",
            "The spot was perfect for camouflage. At least that's what she thought when she picked the spot. She couldn't imagine that anyone would ever be able to see her in these surroundings. So there she sat, confident that she was hidden from the world and safe from danger.\r\n",
            "Unfortunately, she had not anticipated that others may be looking upon her from other angles, and now they were stealthily descending toward her hiding spot.The bowl was filled with fruit. It seemed to be an overabundance of strawberries, but it also included blueberries, raspberries, grapes, and banana slices. This was the meal Sarah had every morning to start her day since she could remember.\r\n",
            "Why she decided to add chocolate as an option today was still a bit of a surprise, but she had been in the process of deciding she wanted to change her routine. This was a baby step to begin that start.\n",
            "\n",
            "\n",
            "Content of the last two files:\n",
            " Here's the thing. She doesn't have anything to prove, but she is going to anyway. That's just her character.\r\n",
            "She knows she doesn't have to, but she still will just to show you that she can. Doubt her more and she'll prove she can again. We all already know this and you will too.\r\n",
            "The spot was perfect for camouflage. At least that's what she thought when she picked the spot. She couldn't imagine that anyone would ever be able to see her in these surroundings. So there she sat, confident that she was hidden from the world and safe from danger.\r\n",
            "Unfortunately, she had not anticipated that others may be looking upon her from other angles, and now they were stealthily descending toward her hiding spot.The bowl was filled with fruit. It seemed to be an overabundance of strawberries, but it also included blueberries, raspberries, grapes, and banana slices. This was the meal Sarah had every morning to start her day since she could remember.\r\n",
            "Why she decided to add chocolate as an option today was still a bit of a surprise, but she had been in the process of deciding she wanted to change her routine. This was a baby step to begin that start.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"File IDs:\", corpus.fileids())\n",
        "print(\"Words in the corpus:\", corpus.words())\n",
        "print(\"Words in the third file:\", corpus.words('file3.txt'))\n",
        "print(\"Number of words in the corpus:\", len(corpus.words()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr1o8eY6MsgM",
        "outputId": "abbe2028-7430-4bc2-fccb-0553bdb0c4f2"
      },
      "execution_count": 394,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File IDs: ['file1.txt', 'file2.txt', 'file3.txt']\n",
            "Words in the corpus: ['The', 'red', 'glint', 'of', 'paint', 'sparkled', ...]\n",
            "Words in the third file: ['The', 'bowl', 'was', 'filled', 'with', 'fruit', '.', ...]\n",
            "Number of words in the corpus: 397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentences in the corpus:\", corpus.sents())\n",
        "print(\"Sentences in the third file:\", corpus.sents('file3.txt'))\n",
        "print(\"Number of sentences in the corpus:\", len(corpus.sents()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2VnXOFm7OwW",
        "outputId": "77705860-8c91-4b7d-8a15-94abba40715b"
      },
      "execution_count": 395,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences in the corpus: [['The', 'red', 'glint', 'of', 'paint', 'sparkled', 'under', 'the', 'sun', '.'], ['He', 'had', 'dreamed', 'of', 'owning', 'this', 'car', 'since', 'he', 'was', 'ten', ',', 'and', 'that', 'dream', 'had', 'become', 'a', 'reality', 'less', 'than', 'a', 'year', 'ago', '.'], ...]\n",
            "Sentences in the third file: [['The', 'bowl', 'was', 'filled', 'with', 'fruit', '.'], ['It', 'seemed', 'to', 'be', 'an', 'overabundance', 'of', 'strawberries', ',', 'but', 'it', 'also', 'included', 'blueberries', ',', 'raspberries', ',', 'grapes', ',', 'and', 'banana', 'slices', '.'], ...]\n",
            "Number of sentences in the corpus: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = nltk.word_tokenize(str(corpus.words('file1.txt')))\n",
        "print(\"The number of words in the first sentence of file 1 is:\", len(words))\n",
        "words = nltk.word_tokenize(str(corpus.words('file2.txt')))\n",
        "print(\"The number of words in the first sentence of file 2 is:\", len(words))\n",
        "words = nltk.word_tokenize(str(corpus.words('file3.txt')))\n",
        "print(\"The number of words in the first sentence of file 3 is:\", len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH_U7JWlNJ6G",
        "outputId": "f320e019-1e24-41eb-9f32-1c2156d1aeed"
      },
      "execution_count": 396,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of words in the first sentence of file 1 is: 21\n",
            "The number of words in the first sentence of file 2 is: 29\n",
            "The number of words in the first sentence of file 3 is: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpustext = \"The red glint of paint sparkled under the sun. He had dreamed of owning this car since he was ten, and that dream had become a reality less than a year ago. It was his baby and he spent hours caring for it, pampering it, and fondling over it. She knew this all too well, and that's exactly why she had taken a sledge hammer to it. She was in a hurry. Not the standard hurry when you're in a rush to get someplace, but a frantic hurry. The type of hurry where a few seconds could mean life or death. She raced down the road ignoring speed limits and weaving between cars. She was only a few minutes away when traffic came to a dead standstill on the road ahead. Here's the thing. She doesn't have anything to prove, but she is going to anyway. That's just her character. She knows she doesn't have to, but she still will just to show you that she can. Doubt her more and she'll prove she can again. We all already know this and you will too. The spot was perfect for camouflage. At least that's what she thought when she picked the spot. She couldn't imagine that anyone would ever be able to see her in these surroundings. So there she sat, confident that she was hidden from the world and safe from danger. Unfortunately, she had not anticipated that others may be looking upon her from other angles, and now they were stealthily descending toward her hiding spot. The bowl was filled with fruit. It seemed to be an overabundance of strawberries, but it also included blueberries, raspberries, grapes, and banana slices. This was the meal Sarah had every morning to start her day since she could remember. Why she decided to add chocolate as an option today was still a bit of a surprise, but she had been in the process of deciding she wanted to change her routine. This was a baby step to begin that start.\"\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "print(\"Word tokens:\", word_tokenize(corpustext))\n",
        "print(\"Sentence tokens:\", sent_tokenize(corpustext))\n",
        "print(\"\\n\")\n",
        "words = nltk.word_tokenize(corpustext)\n",
        "print(\"Number of words:\", len(words))\n",
        "sents = nltk.sent_tokenize(corpustext)\n",
        "print(\"Number of sentences:\", len(sents))\n",
        "average_tokens = round(len(words)/len(sents))\n",
        "print(\"Average number of tokens per sentence:\", average_tokens)\n",
        "unique_tokens = set(words)\n",
        "print(\"Number of unique tokens:\", len(unique_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2W0EdvRUm8E",
        "outputId": "3926414d-29a4-443e-b079-4e05bb9d117f"
      },
      "execution_count": 397,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokens: ['The', 'red', 'glint', 'of', 'paint', 'sparkled', 'under', 'the', 'sun', '.', 'He', 'had', 'dreamed', 'of', 'owning', 'this', 'car', 'since', 'he', 'was', 'ten', ',', 'and', 'that', 'dream', 'had', 'become', 'a', 'reality', 'less', 'than', 'a', 'year', 'ago', '.', 'It', 'was', 'his', 'baby', 'and', 'he', 'spent', 'hours', 'caring', 'for', 'it', ',', 'pampering', 'it', ',', 'and', 'fondling', 'over', 'it', '.', 'She', 'knew', 'this', 'all', 'too', 'well', ',', 'and', 'that', \"'s\", 'exactly', 'why', 'she', 'had', 'taken', 'a', 'sledge', 'hammer', 'to', 'it', '.', 'She', 'was', 'in', 'a', 'hurry', '.', 'Not', 'the', 'standard', 'hurry', 'when', 'you', \"'re\", 'in', 'a', 'rush', 'to', 'get', 'someplace', ',', 'but', 'a', 'frantic', 'hurry', '.', 'The', 'type', 'of', 'hurry', 'where', 'a', 'few', 'seconds', 'could', 'mean', 'life', 'or', 'death', '.', 'She', 'raced', 'down', 'the', 'road', 'ignoring', 'speed', 'limits', 'and', 'weaving', 'between', 'cars', '.', 'She', 'was', 'only', 'a', 'few', 'minutes', 'away', 'when', 'traffic', 'came', 'to', 'a', 'dead', 'standstill', 'on', 'the', 'road', 'ahead', '.', 'Here', \"'s\", 'the', 'thing', '.', 'She', 'does', \"n't\", 'have', 'anything', 'to', 'prove', ',', 'but', 'she', 'is', 'going', 'to', 'anyway', '.', 'That', \"'s\", 'just', 'her', 'character', '.', 'She', 'knows', 'she', 'does', \"n't\", 'have', 'to', ',', 'but', 'she', 'still', 'will', 'just', 'to', 'show', 'you', 'that', 'she', 'can', '.', 'Doubt', 'her', 'more', 'and', 'she', \"'ll\", 'prove', 'she', 'can', 'again', '.', 'We', 'all', 'already', 'know', 'this', 'and', 'you', 'will', 'too', '.', 'The', 'spot', 'was', 'perfect', 'for', 'camouflage', '.', 'At', 'least', 'that', \"'s\", 'what', 'she', 'thought', 'when', 'she', 'picked', 'the', 'spot', '.', 'She', 'could', \"n't\", 'imagine', 'that', 'anyone', 'would', 'ever', 'be', 'able', 'to', 'see', 'her', 'in', 'these', 'surroundings', '.', 'So', 'there', 'she', 'sat', ',', 'confident', 'that', 'she', 'was', 'hidden', 'from', 'the', 'world', 'and', 'safe', 'from', 'danger', '.', 'Unfortunately', ',', 'she', 'had', 'not', 'anticipated', 'that', 'others', 'may', 'be', 'looking', 'upon', 'her', 'from', 'other', 'angles', ',', 'and', 'now', 'they', 'were', 'stealthily', 'descending', 'toward', 'her', 'hiding', 'spot', '.', 'The', 'bowl', 'was', 'filled', 'with', 'fruit', '.', 'It', 'seemed', 'to', 'be', 'an', 'overabundance', 'of', 'strawberries', ',', 'but', 'it', 'also', 'included', 'blueberries', ',', 'raspberries', ',', 'grapes', ',', 'and', 'banana', 'slices', '.', 'This', 'was', 'the', 'meal', 'Sarah', 'had', 'every', 'morning', 'to', 'start', 'her', 'day', 'since', 'she', 'could', 'remember', '.', 'Why', 'she', 'decided', 'to', 'add', 'chocolate', 'as', 'an', 'option', 'today', 'was', 'still', 'a', 'bit', 'of', 'a', 'surprise', ',', 'but', 'she', 'had', 'been', 'in', 'the', 'process', 'of', 'deciding', 'she', 'wanted', 'to', 'change', 'her', 'routine', '.', 'This', 'was', 'a', 'baby', 'step', 'to', 'begin', 'that', 'start', '.']\n",
            "Sentence tokens: ['The red glint of paint sparkled under the sun.', 'He had dreamed of owning this car since he was ten, and that dream had become a reality less than a year ago.', 'It was his baby and he spent hours caring for it, pampering it, and fondling over it.', \"She knew this all too well, and that's exactly why she had taken a sledge hammer to it.\", 'She was in a hurry.', \"Not the standard hurry when you're in a rush to get someplace, but a frantic hurry.\", 'The type of hurry where a few seconds could mean life or death.', 'She raced down the road ignoring speed limits and weaving between cars.', 'She was only a few minutes away when traffic came to a dead standstill on the road ahead.', \"Here's the thing.\", \"She doesn't have anything to prove, but she is going to anyway.\", \"That's just her character.\", \"She knows she doesn't have to, but she still will just to show you that she can.\", \"Doubt her more and she'll prove she can again.\", 'We all already know this and you will too.', 'The spot was perfect for camouflage.', \"At least that's what she thought when she picked the spot.\", \"She couldn't imagine that anyone would ever be able to see her in these surroundings.\", 'So there she sat, confident that she was hidden from the world and safe from danger.', 'Unfortunately, she had not anticipated that others may be looking upon her from other angles, and now they were stealthily descending toward her hiding spot.', 'The bowl was filled with fruit.', 'It seemed to be an overabundance of strawberries, but it also included blueberries, raspberries, grapes, and banana slices.', 'This was the meal Sarah had every morning to start her day since she could remember.', 'Why she decided to add chocolate as an option today was still a bit of a surprise, but she had been in the process of deciding she wanted to change her routine.', 'This was a baby step to begin that start.']\n",
            "\n",
            "\n",
            "Number of words: 388\n",
            "Number of sentences: 25\n",
            "Average number of tokens per sentence: 16\n",
            "Number of unique tokens: 202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lowercase:\", corpustext.lower())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-VuGVJ-fqgF",
        "outputId": "45d841f8-57f5-401f-d869-1bbaa4c294be"
      },
      "execution_count": 436,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowercase: the red glint of paint sparkled under the sun. he had dreamed of owning this car since he was ten, and that dream had become a reality less than a year ago. it was his baby and he spent hours caring for it, pampering it, and fondling over it. she knew this all too well, and that's exactly why she had taken a sledge hammer to it. she was in a hurry. not the standard hurry when you're in a rush to get someplace, but a frantic hurry. the type of hurry where a few seconds could mean life or death. she raced down the road ignoring speed limits and weaving between cars. she was only a few minutes away when traffic came to a dead standstill on the road ahead. here's the thing. she doesn't have anything to prove, but she is going to anyway. that's just her character. she knows she doesn't have to, but she still will just to show you that she can. doubt her more and she'll prove she can again. we all already know this and you will too. the spot was perfect for camouflage. at least that's what she thought when she picked the spot. she couldn't imagine that anyone would ever be able to see her in these surroundings. so there she sat, confident that she was hidden from the world and safe from danger. unfortunately, she had not anticipated that others may be looking upon her from other angles, and now they were stealthily descending toward her hiding spot. the bowl was filled with fruit. it seemed to be an overabundance of strawberries, but it also included blueberries, raspberries, grapes, and banana slices. this was the meal sarah had every morning to start her day since she could remember. why she decided to add chocolate as an option today was still a bit of a surprise, but she had been in the process of deciding she wanted to change her routine. this was a baby step to begin that start.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "final_tokens = []\n",
        "for each in words:\n",
        " if each not in stop_words:\n",
        "    final_tokens.append(each)\n",
        "print(\"Number of tokens after removing stopwords:\", len((final_tokens)))\n",
        "print(\"After removing stopwords:\", final_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU4Jl1YkV3dT",
        "outputId": "7b034f24-383e-4ff4-d071-c65985806c57"
      },
      "execution_count": 430,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens after removing stopwords: 215\n",
            "After removing stopwords: ['The', 'red', 'glint', 'paint', 'sparkled', 'sun', '.', 'He', 'dreamed', 'owning', 'car', 'since', 'ten', ',', 'dream', 'become', 'reality', 'less', 'year', 'ago', '.', 'It', 'baby', 'spent', 'hours', 'caring', ',', 'pampering', ',', 'fondling', '.', 'She', 'knew', 'well', ',', \"'s\", 'exactly', 'taken', 'sledge', 'hammer', '.', 'She', 'hurry', '.', 'Not', 'standard', 'hurry', \"'re\", 'rush', 'get', 'someplace', ',', 'frantic', 'hurry', '.', 'The', 'type', 'hurry', 'seconds', 'could', 'mean', 'life', 'death', '.', 'She', 'raced', 'road', 'ignoring', 'speed', 'limits', 'weaving', 'cars', '.', 'She', 'minutes', 'away', 'traffic', 'came', 'dead', 'standstill', 'road', 'ahead', '.', 'Here', \"'s\", 'thing', '.', 'She', \"n't\", 'anything', 'prove', ',', 'going', 'anyway', '.', 'That', \"'s\", 'character', '.', 'She', 'knows', \"n't\", ',', 'still', 'show', '.', 'Doubt', \"'ll\", 'prove', '.', 'We', 'already', 'know', '.', 'The', 'spot', 'perfect', 'camouflage', '.', 'At', 'least', \"'s\", 'thought', 'picked', 'spot', '.', 'She', 'could', \"n't\", 'imagine', 'anyone', 'would', 'ever', 'able', 'see', 'surroundings', '.', 'So', 'sat', ',', 'confident', 'hidden', 'world', 'safe', 'danger', '.', 'Unfortunately', ',', 'anticipated', 'others', 'may', 'looking', 'upon', 'angles', ',', 'stealthily', 'descending', 'toward', 'hiding', 'spot', '.', 'The', 'bowl', 'filled', 'fruit', '.', 'It', 'seemed', 'overabundance', 'strawberries', ',', 'also', 'included', 'blueberries', ',', 'raspberries', ',', 'grapes', ',', 'banana', 'slices', '.', 'This', 'meal', 'Sarah', 'every', 'morning', 'start', 'day', 'since', 'could', 'remember', '.', 'Why', 'decided', 'add', 'chocolate', 'option', 'today', 'still', 'bit', 'surprise', ',', 'process', 'deciding', 'wanted', 'change', 'routine', '.', 'This', 'baby', 'step', 'begin', 'start', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer \n",
        "stemmer_ss = SnowballStemmer(\"english\")   \n",
        "stemmed_words_ss = [stemmer_ss.stem(word) for word in words]\n",
        "print(\"Snowball stemmed words:\", stemmed_words_ss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lmufg_WWegI",
        "outputId": "202938b6-13bd-4e96-f3dd-cfcff9583c1f"
      },
      "execution_count": 401,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Snowball stemmed words: ['the', 'red', 'glint', 'of', 'paint', 'sparkl', 'under', 'the', 'sun', '.', 'he', 'had', 'dream', 'of', 'own', 'this', 'car', 'sinc', 'he', 'was', 'ten', ',', 'and', 'that', 'dream', 'had', 'becom', 'a', 'realiti', 'less', 'than', 'a', 'year', 'ago', '.', 'it', 'was', 'his', 'babi', 'and', 'he', 'spent', 'hour', 'care', 'for', 'it', ',', 'pamper', 'it', ',', 'and', 'fondl', 'over', 'it', '.', 'she', 'knew', 'this', 'all', 'too', 'well', ',', 'and', 'that', \"'s\", 'exact', 'whi', 'she', 'had', 'taken', 'a', 'sledg', 'hammer', 'to', 'it', '.', 'she', 'was', 'in', 'a', 'hurri', '.', 'not', 'the', 'standard', 'hurri', 'when', 'you', 're', 'in', 'a', 'rush', 'to', 'get', 'someplac', ',', 'but', 'a', 'frantic', 'hurri', '.', 'the', 'type', 'of', 'hurri', 'where', 'a', 'few', 'second', 'could', 'mean', 'life', 'or', 'death', '.', 'she', 'race', 'down', 'the', 'road', 'ignor', 'speed', 'limit', 'and', 'weav', 'between', 'car', '.', 'she', 'was', 'onli', 'a', 'few', 'minut', 'away', 'when', 'traffic', 'came', 'to', 'a', 'dead', 'standstil', 'on', 'the', 'road', 'ahead', '.', 'here', \"'s\", 'the', 'thing', '.', 'she', 'doe', \"n't\", 'have', 'anyth', 'to', 'prove', ',', 'but', 'she', 'is', 'go', 'to', 'anyway', '.', 'that', \"'s\", 'just', 'her', 'charact', '.', 'she', 'know', 'she', 'doe', \"n't\", 'have', 'to', ',', 'but', 'she', 'still', 'will', 'just', 'to', 'show', 'you', 'that', 'she', 'can', '.', 'doubt', 'her', 'more', 'and', 'she', 'll', 'prove', 'she', 'can', 'again', '.', 'we', 'all', 'alreadi', 'know', 'this', 'and', 'you', 'will', 'too', '.', 'the', 'spot', 'was', 'perfect', 'for', 'camouflag', '.', 'at', 'least', 'that', \"'s\", 'what', 'she', 'thought', 'when', 'she', 'pick', 'the', 'spot', '.', 'she', 'could', \"n't\", 'imagin', 'that', 'anyon', 'would', 'ever', 'be', 'abl', 'to', 'see', 'her', 'in', 'these', 'surround', '.', 'so', 'there', 'she', 'sat', ',', 'confid', 'that', 'she', 'was', 'hidden', 'from', 'the', 'world', 'and', 'safe', 'from', 'danger', '.', 'unfortun', ',', 'she', 'had', 'not', 'anticip', 'that', 'other', 'may', 'be', 'look', 'upon', 'her', 'from', 'other', 'angl', ',', 'and', 'now', 'they', 'were', 'stealthili', 'descend', 'toward', 'her', 'hide', 'spot', '.', 'the', 'bowl', 'was', 'fill', 'with', 'fruit', '.', 'it', 'seem', 'to', 'be', 'an', 'overabund', 'of', 'strawberri', ',', 'but', 'it', 'also', 'includ', 'blueberri', ',', 'raspberri', ',', 'grape', ',', 'and', 'banana', 'slice', '.', 'this', 'was', 'the', 'meal', 'sarah', 'had', 'everi', 'morn', 'to', 'start', 'her', 'day', 'sinc', 'she', 'could', 'rememb', '.', 'whi', 'she', 'decid', 'to', 'add', 'chocol', 'as', 'an', 'option', 'today', 'was', 'still', 'a', 'bit', 'of', 'a', 'surpris', ',', 'but', 'she', 'had', 'been', 'in', 'the', 'process', 'of', 'decid', 'she', 'want', 'to', 'chang', 'her', 'routin', '.', 'this', 'was', 'a', 'babi', 'step', 'to', 'begin', 'that', 'start', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stemSentence(sentence):\n",
        "    token_words = word_tokenize(sentence) \n",
        "    stem_sentence = []\n",
        "    for word in token_words:\n",
        "        stem_sentence.append(stemmer_ss.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    return \"\".join(stem_sentence)\n",
        "stemmed_sentence = stemSentence(corpustext)\n",
        "print(\"Snowball stemmed sentence:\", stemmed_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTnFsnClauii",
        "outputId": "98e38310-7dfd-48b6-d963-2c194329f0df"
      },
      "execution_count": 410,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Snowball stemmed sentence: the red glint of paint sparkl under the sun . he had dream of own this car sinc he was ten , and that dream had becom a realiti less than a year ago . it was his babi and he spent hour care for it , pamper it , and fondl over it . she knew this all too well , and that 's exact whi she had taken a sledg hammer to it . she was in a hurri . not the standard hurri when you re in a rush to get someplac , but a frantic hurri . the type of hurri where a few second could mean life or death . she race down the road ignor speed limit and weav between car . she was onli a few minut away when traffic came to a dead standstil on the road ahead . here 's the thing . she doe n't have anyth to prove , but she is go to anyway . that 's just her charact . she know she doe n't have to , but she still will just to show you that she can . doubt her more and she ll prove she can again . we all alreadi know this and you will too . the spot was perfect for camouflag . at least that 's what she thought when she pick the spot . she could n't imagin that anyon would ever be abl to see her in these surround . so there she sat , confid that she was hidden from the world and safe from danger . unfortun , she had not anticip that other may be look upon her from other angl , and now they were stealthili descend toward her hide spot . the bowl was fill with fruit . it seem to be an overabund of strawberri , but it also includ blueberri , raspberri , grape , and banana slice . this was the meal sarah had everi morn to start her day sinc she could rememb . whi she decid to add chocol as an option today was still a bit of a surpris , but she had been in the process of decid she want to chang her routin . this was a babi step to begin that start . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()   \n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words] \n",
        "print(\"Lemmatized words:\", lemmatized_words)  \n",
        "lemmatized_words_pos = [lemmatizer.lemmatize(word, pos = \"v\") for word in words]\n",
        "print(\"Lemmatized words using a POS tag:\", lemmatized_words_pos) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywe3ch_Xbb9s",
        "outputId": "cf4b5d8e-f41c-4594-834f-73fc02c50ebd"
      },
      "execution_count": 415,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized words: ['The', 'red', 'glint', 'of', 'paint', 'sparkled', 'under', 'the', 'sun', '.', 'He', 'had', 'dreamed', 'of', 'owning', 'this', 'car', 'since', 'he', 'wa', 'ten', ',', 'and', 'that', 'dream', 'had', 'become', 'a', 'reality', 'le', 'than', 'a', 'year', 'ago', '.', 'It', 'wa', 'his', 'baby', 'and', 'he', 'spent', 'hour', 'caring', 'for', 'it', ',', 'pampering', 'it', ',', 'and', 'fondling', 'over', 'it', '.', 'She', 'knew', 'this', 'all', 'too', 'well', ',', 'and', 'that', \"'s\", 'exactly', 'why', 'she', 'had', 'taken', 'a', 'sledge', 'hammer', 'to', 'it', '.', 'She', 'wa', 'in', 'a', 'hurry', '.', 'Not', 'the', 'standard', 'hurry', 'when', 'you', \"'re\", 'in', 'a', 'rush', 'to', 'get', 'someplace', ',', 'but', 'a', 'frantic', 'hurry', '.', 'The', 'type', 'of', 'hurry', 'where', 'a', 'few', 'second', 'could', 'mean', 'life', 'or', 'death', '.', 'She', 'raced', 'down', 'the', 'road', 'ignoring', 'speed', 'limit', 'and', 'weaving', 'between', 'car', '.', 'She', 'wa', 'only', 'a', 'few', 'minute', 'away', 'when', 'traffic', 'came', 'to', 'a', 'dead', 'standstill', 'on', 'the', 'road', 'ahead', '.', 'Here', \"'s\", 'the', 'thing', '.', 'She', 'doe', \"n't\", 'have', 'anything', 'to', 'prove', ',', 'but', 'she', 'is', 'going', 'to', 'anyway', '.', 'That', \"'s\", 'just', 'her', 'character', '.', 'She', 'know', 'she', 'doe', \"n't\", 'have', 'to', ',', 'but', 'she', 'still', 'will', 'just', 'to', 'show', 'you', 'that', 'she', 'can', '.', 'Doubt', 'her', 'more', 'and', 'she', \"'ll\", 'prove', 'she', 'can', 'again', '.', 'We', 'all', 'already', 'know', 'this', 'and', 'you', 'will', 'too', '.', 'The', 'spot', 'wa', 'perfect', 'for', 'camouflage', '.', 'At', 'least', 'that', \"'s\", 'what', 'she', 'thought', 'when', 'she', 'picked', 'the', 'spot', '.', 'She', 'could', \"n't\", 'imagine', 'that', 'anyone', 'would', 'ever', 'be', 'able', 'to', 'see', 'her', 'in', 'these', 'surroundings', '.', 'So', 'there', 'she', 'sat', ',', 'confident', 'that', 'she', 'wa', 'hidden', 'from', 'the', 'world', 'and', 'safe', 'from', 'danger', '.', 'Unfortunately', ',', 'she', 'had', 'not', 'anticipated', 'that', 'others', 'may', 'be', 'looking', 'upon', 'her', 'from', 'other', 'angle', ',', 'and', 'now', 'they', 'were', 'stealthily', 'descending', 'toward', 'her', 'hiding', 'spot', '.', 'The', 'bowl', 'wa', 'filled', 'with', 'fruit', '.', 'It', 'seemed', 'to', 'be', 'an', 'overabundance', 'of', 'strawberry', ',', 'but', 'it', 'also', 'included', 'blueberry', ',', 'raspberry', ',', 'grape', ',', 'and', 'banana', 'slice', '.', 'This', 'wa', 'the', 'meal', 'Sarah', 'had', 'every', 'morning', 'to', 'start', 'her', 'day', 'since', 'she', 'could', 'remember', '.', 'Why', 'she', 'decided', 'to', 'add', 'chocolate', 'a', 'an', 'option', 'today', 'wa', 'still', 'a', 'bit', 'of', 'a', 'surprise', ',', 'but', 'she', 'had', 'been', 'in', 'the', 'process', 'of', 'deciding', 'she', 'wanted', 'to', 'change', 'her', 'routine', '.', 'This', 'wa', 'a', 'baby', 'step', 'to', 'begin', 'that', 'start', '.']\n",
            "Lemmatized words using a POS tag: ['The', 'red', 'glint', 'of', 'paint', 'sparkle', 'under', 'the', 'sun', '.', 'He', 'have', 'dream', 'of', 'own', 'this', 'car', 'since', 'he', 'be', 'ten', ',', 'and', 'that', 'dream', 'have', 'become', 'a', 'reality', 'less', 'than', 'a', 'year', 'ago', '.', 'It', 'be', 'his', 'baby', 'and', 'he', 'spend', 'hours', 'care', 'for', 'it', ',', 'pamper', 'it', ',', 'and', 'fondle', 'over', 'it', '.', 'She', 'know', 'this', 'all', 'too', 'well', ',', 'and', 'that', \"'s\", 'exactly', 'why', 'she', 'have', 'take', 'a', 'sledge', 'hammer', 'to', 'it', '.', 'She', 'be', 'in', 'a', 'hurry', '.', 'Not', 'the', 'standard', 'hurry', 'when', 'you', \"'re\", 'in', 'a', 'rush', 'to', 'get', 'someplace', ',', 'but', 'a', 'frantic', 'hurry', '.', 'The', 'type', 'of', 'hurry', 'where', 'a', 'few', 'second', 'could', 'mean', 'life', 'or', 'death', '.', 'She', 'race', 'down', 'the', 'road', 'ignore', 'speed', 'limit', 'and', 'weave', 'between', 'cars', '.', 'She', 'be', 'only', 'a', 'few', 'minutes', 'away', 'when', 'traffic', 'come', 'to', 'a', 'dead', 'standstill', 'on', 'the', 'road', 'ahead', '.', 'Here', \"'s\", 'the', 'thing', '.', 'She', 'do', \"n't\", 'have', 'anything', 'to', 'prove', ',', 'but', 'she', 'be', 'go', 'to', 'anyway', '.', 'That', \"'s\", 'just', 'her', 'character', '.', 'She', 'know', 'she', 'do', \"n't\", 'have', 'to', ',', 'but', 'she', 'still', 'will', 'just', 'to', 'show', 'you', 'that', 'she', 'can', '.', 'Doubt', 'her', 'more', 'and', 'she', \"'ll\", 'prove', 'she', 'can', 'again', '.', 'We', 'all', 'already', 'know', 'this', 'and', 'you', 'will', 'too', '.', 'The', 'spot', 'be', 'perfect', 'for', 'camouflage', '.', 'At', 'least', 'that', \"'s\", 'what', 'she', 'think', 'when', 'she', 'pick', 'the', 'spot', '.', 'She', 'could', \"n't\", 'imagine', 'that', 'anyone', 'would', 'ever', 'be', 'able', 'to', 'see', 'her', 'in', 'these', 'surround', '.', 'So', 'there', 'she', 'sit', ',', 'confident', 'that', 'she', 'be', 'hide', 'from', 'the', 'world', 'and', 'safe', 'from', 'danger', '.', 'Unfortunately', ',', 'she', 'have', 'not', 'anticipate', 'that', 'others', 'may', 'be', 'look', 'upon', 'her', 'from', 'other', 'angle', ',', 'and', 'now', 'they', 'be', 'stealthily', 'descend', 'toward', 'her', 'hide', 'spot', '.', 'The', 'bowl', 'be', 'fill', 'with', 'fruit', '.', 'It', 'seem', 'to', 'be', 'an', 'overabundance', 'of', 'strawberries', ',', 'but', 'it', 'also', 'include', 'blueberries', ',', 'raspberries', ',', 'grapes', ',', 'and', 'banana', 'slice', '.', 'This', 'be', 'the', 'meal', 'Sarah', 'have', 'every', 'morning', 'to', 'start', 'her', 'day', 'since', 'she', 'could', 'remember', '.', 'Why', 'she', 'decide', 'to', 'add', 'chocolate', 'as', 'an', 'option', 'today', 'be', 'still', 'a', 'bite', 'of', 'a', 'surprise', ',', 'but', 'she', 'have', 'be', 'in', 'the', 'process', 'of', 'decide', 'she', 'want', 'to', 'change', 'her', 'routine', '.', 'This', 'be', 'a', 'baby', 'step', 'to', 'begin', 'that', 'start', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatizeSentence(sentence):\n",
        "    token_words = word_tokenize(sentence) \n",
        "    lemma_sentence = []\n",
        "    for word in token_words:\n",
        "        lemma_sentence.append(lemmatizer.lemmatize(word))\n",
        "        lemma_sentence.append(\" \")\n",
        "    return \"\".join(lemma_sentence)\n",
        "lemma_sentence = lemmatizeSentence(corpustext)\n",
        "print(\"Lemmatized sentence:\", lemma_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ur0WliIb7sS",
        "outputId": "958513da-4991-4134-d21d-df6da1991a5b"
      },
      "execution_count": 419,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized sentence: The red glint of paint sparkled under the sun . He had dreamed of owning this car since he wa ten , and that dream had become a reality le than a year ago . It wa his baby and he spent hour caring for it , pampering it , and fondling over it . She knew this all too well , and that 's exactly why she had taken a sledge hammer to it . She wa in a hurry . Not the standard hurry when you 're in a rush to get someplace , but a frantic hurry . The type of hurry where a few second could mean life or death . She raced down the road ignoring speed limit and weaving between car . She wa only a few minute away when traffic came to a dead standstill on the road ahead . Here 's the thing . She doe n't have anything to prove , but she is going to anyway . That 's just her character . She know she doe n't have to , but she still will just to show you that she can . Doubt her more and she 'll prove she can again . We all already know this and you will too . The spot wa perfect for camouflage . At least that 's what she thought when she picked the spot . She could n't imagine that anyone would ever be able to see her in these surroundings . So there she sat , confident that she wa hidden from the world and safe from danger . Unfortunately , she had not anticipated that others may be looking upon her from other angle , and now they were stealthily descending toward her hiding spot . The bowl wa filled with fruit . It seemed to be an overabundance of strawberry , but it also included blueberry , raspberry , grape , and banana slice . This wa the meal Sarah had every morning to start her day since she could remember . Why she decided to add chocolate a an option today wa still a bit of a surprise , but she had been in the process of deciding she wanted to change her routine . This wa a baby step to begin that start . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = nltk.sent_tokenize(corpustext)\n",
        "print(\"POS tagged:\")\n",
        "for sent in sentence:\n",
        "\t print(nltk.pos_tag(nltk.word_tokenize(sent)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWJ1c7oEfP0b",
        "outputId": "4a4f6a2f-8f4c-4446-c268-4b5f56d93a86"
      },
      "execution_count": 435,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tagged:\n",
            "[('The', 'DT'), ('red', 'JJ'), ('glint', 'NN'), ('of', 'IN'), ('paint', 'NN'), ('sparkled', 'VBN'), ('under', 'IN'), ('the', 'DT'), ('sun', 'NN'), ('.', '.')]\n",
            "[('He', 'PRP'), ('had', 'VBD'), ('dreamed', 'VBN'), ('of', 'IN'), ('owning', 'VBG'), ('this', 'DT'), ('car', 'NN'), ('since', 'IN'), ('he', 'PRP'), ('was', 'VBD'), ('ten', 'VBN'), (',', ','), ('and', 'CC'), ('that', 'IN'), ('dream', 'NN'), ('had', 'VBD'), ('become', 'VBN'), ('a', 'DT'), ('reality', 'NN'), ('less', 'JJR'), ('than', 'IN'), ('a', 'DT'), ('year', 'NN'), ('ago', 'RB'), ('.', '.')]\n",
            "[('It', 'PRP'), ('was', 'VBD'), ('his', 'PRP$'), ('baby', 'NN'), ('and', 'CC'), ('he', 'PRP'), ('spent', 'VBD'), ('hours', 'NNS'), ('caring', 'VBG'), ('for', 'IN'), ('it', 'PRP'), (',', ','), ('pampering', 'VBG'), ('it', 'PRP'), (',', ','), ('and', 'CC'), ('fondling', 'VBG'), ('over', 'IN'), ('it', 'PRP'), ('.', '.')]\n",
            "[('She', 'PRP'), ('knew', 'VBD'), ('this', 'DT'), ('all', 'DT'), ('too', 'RB'), ('well', 'RB'), (',', ','), ('and', 'CC'), ('that', 'DT'), (\"'s\", 'VBZ'), ('exactly', 'RB'), ('why', 'WRB'), ('she', 'PRP'), ('had', 'VBD'), ('taken', 'VBN'), ('a', 'DT'), ('sledge', 'NN'), ('hammer', 'NN'), ('to', 'TO'), ('it', 'PRP'), ('.', '.')]\n",
            "[('She', 'PRP'), ('was', 'VBD'), ('in', 'IN'), ('a', 'DT'), ('hurry', 'NN'), ('.', '.')]\n",
            "[('Not', 'RB'), ('the', 'DT'), ('standard', 'NN'), ('hurry', 'NN'), ('when', 'WRB'), ('you', 'PRP'), (\"'re\", 'VBP'), ('in', 'IN'), ('a', 'DT'), ('rush', 'NN'), ('to', 'TO'), ('get', 'VB'), ('someplace', 'NN'), (',', ','), ('but', 'CC'), ('a', 'DT'), ('frantic', 'JJ'), ('hurry', 'NN'), ('.', '.')]\n",
            "[('The', 'DT'), ('type', 'NN'), ('of', 'IN'), ('hurry', 'NN'), ('where', 'WRB'), ('a', 'DT'), ('few', 'JJ'), ('seconds', 'NNS'), ('could', 'MD'), ('mean', 'VB'), ('life', 'NN'), ('or', 'CC'), ('death', 'NN'), ('.', '.')]\n",
            "[('She', 'PRP'), ('raced', 'VBD'), ('down', 'IN'), ('the', 'DT'), ('road', 'NN'), ('ignoring', 'VBG'), ('speed', 'NN'), ('limits', 'NNS'), ('and', 'CC'), ('weaving', 'VBG'), ('between', 'IN'), ('cars', 'NNS'), ('.', '.')]\n",
            "[('She', 'PRP'), ('was', 'VBD'), ('only', 'RB'), ('a', 'DT'), ('few', 'JJ'), ('minutes', 'NNS'), ('away', 'RB'), ('when', 'WRB'), ('traffic', 'NN'), ('came', 'VBD'), ('to', 'TO'), ('a', 'DT'), ('dead', 'JJ'), ('standstill', 'NN'), ('on', 'IN'), ('the', 'DT'), ('road', 'NN'), ('ahead', 'RB'), ('.', '.')]\n",
            "[('Here', 'RB'), (\"'s\", 'VBZ'), ('the', 'DT'), ('thing', 'NN'), ('.', '.')]\n",
            "[('She', 'PRP'), ('does', 'VBZ'), (\"n't\", 'RB'), ('have', 'VB'), ('anything', 'NN'), ('to', 'TO'), ('prove', 'VB'), (',', ','), ('but', 'CC'), ('she', 'PRP'), ('is', 'VBZ'), ('going', 'VBG'), ('to', 'TO'), ('anyway', 'VB'), ('.', '.')]\n",
            "[('That', 'DT'), (\"'s\", 'VBZ'), ('just', 'RB'), ('her', 'PRP$'), ('character', 'NN'), ('.', '.')]\n",
            "[('She', 'PRP'), ('knows', 'VBZ'), ('she', 'PRP'), ('does', 'VBZ'), (\"n't\", 'RB'), ('have', 'VB'), ('to', 'TO'), (',', ','), ('but', 'CC'), ('she', 'PRP'), ('still', 'RB'), ('will', 'MD'), ('just', 'RB'), ('to', 'TO'), ('show', 'VB'), ('you', 'PRP'), ('that', 'IN'), ('she', 'PRP'), ('can', 'MD'), ('.', '.')]\n",
            "[('Doubt', 'VB'), ('her', 'PRP$'), ('more', 'JJR'), ('and', 'CC'), ('she', 'PRP'), (\"'ll\", 'MD'), ('prove', 'VB'), ('she', 'PRP'), ('can', 'MD'), ('again', 'RB'), ('.', '.')]\n",
            "[('We', 'PRP'), ('all', 'DT'), ('already', 'RB'), ('know', 'VBP'), ('this', 'DT'), ('and', 'CC'), ('you', 'PRP'), ('will', 'MD'), ('too', 'RB'), ('.', '.')]\n",
            "[('The', 'DT'), ('spot', 'NN'), ('was', 'VBD'), ('perfect', 'JJ'), ('for', 'IN'), ('camouflage', 'NN'), ('.', '.')]\n",
            "[('At', 'IN'), ('least', 'JJS'), ('that', 'DT'), (\"'s\", 'VBZ'), ('what', 'WP'), ('she', 'PRP'), ('thought', 'VBD'), ('when', 'WRB'), ('she', 'PRP'), ('picked', 'VBD'), ('the', 'DT'), ('spot', 'NN'), ('.', '.')]\n",
            "[('She', 'PRP'), ('could', 'MD'), (\"n't\", 'RB'), ('imagine', 'VB'), ('that', 'IN'), ('anyone', 'NN'), ('would', 'MD'), ('ever', 'RB'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('see', 'VB'), ('her', 'PRP$'), ('in', 'IN'), ('these', 'DT'), ('surroundings', 'NNS'), ('.', '.')]\n",
            "[('So', 'RB'), ('there', 'EX'), ('she', 'PRP'), ('sat', 'VBD'), (',', ','), ('confident', 'JJ'), ('that', 'IN'), ('she', 'PRP'), ('was', 'VBD'), ('hidden', 'VBN'), ('from', 'IN'), ('the', 'DT'), ('world', 'NN'), ('and', 'CC'), ('safe', 'JJ'), ('from', 'IN'), ('danger', 'NN'), ('.', '.')]\n",
            "[('Unfortunately', 'RB'), (',', ','), ('she', 'PRP'), ('had', 'VBD'), ('not', 'RB'), ('anticipated', 'VBN'), ('that', 'IN'), ('others', 'NNS'), ('may', 'MD'), ('be', 'VB'), ('looking', 'VBG'), ('upon', 'IN'), ('her', 'PRP$'), ('from', 'IN'), ('other', 'JJ'), ('angles', 'NNS'), (',', ','), ('and', 'CC'), ('now', 'RB'), ('they', 'PRP'), ('were', 'VBD'), ('stealthily', 'RB'), ('descending', 'VBG'), ('toward', 'IN'), ('her', 'PRP$'), ('hiding', 'NN'), ('spot', 'NN'), ('.', '.')]\n",
            "[('The', 'DT'), ('bowl', 'NN'), ('was', 'VBD'), ('filled', 'VBN'), ('with', 'IN'), ('fruit', 'NN'), ('.', '.')]\n",
            "[('It', 'PRP'), ('seemed', 'VBD'), ('to', 'TO'), ('be', 'VB'), ('an', 'DT'), ('overabundance', 'NN'), ('of', 'IN'), ('strawberries', 'NNS'), (',', ','), ('but', 'CC'), ('it', 'PRP'), ('also', 'RB'), ('included', 'VBD'), ('blueberries', 'NNS'), (',', ','), ('raspberries', 'NNS'), (',', ','), ('grapes', 'NNS'), (',', ','), ('and', 'CC'), ('banana', 'NN'), ('slices', 'NNS'), ('.', '.')]\n",
            "[('This', 'DT'), ('was', 'VBD'), ('the', 'DT'), ('meal', 'NN'), ('Sarah', 'NNP'), ('had', 'VBD'), ('every', 'DT'), ('morning', 'NN'), ('to', 'TO'), ('start', 'VB'), ('her', 'PRP$'), ('day', 'NN'), ('since', 'IN'), ('she', 'PRP'), ('could', 'MD'), ('remember', 'VB'), ('.', '.')]\n",
            "[('Why', 'WRB'), ('she', 'PRP'), ('decided', 'VBD'), ('to', 'TO'), ('add', 'VB'), ('chocolate', 'NN'), ('as', 'IN'), ('an', 'DT'), ('option', 'NN'), ('today', 'NN'), ('was', 'VBD'), ('still', 'RB'), ('a', 'DT'), ('bit', 'NN'), ('of', 'IN'), ('a', 'DT'), ('surprise', 'NN'), (',', ','), ('but', 'CC'), ('she', 'PRP'), ('had', 'VBD'), ('been', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('deciding', 'VBG'), ('she', 'PRP'), ('wanted', 'VBD'), ('to', 'TO'), ('change', 'VB'), ('her', 'PRP$'), ('routine', 'NN'), ('.', '.')]\n",
            "[('This', 'DT'), ('was', 'VBD'), ('a', 'DT'), ('baby', 'JJ'), ('step', 'NN'), ('to', 'TO'), ('begin', 'VB'), ('that', 'DT'), ('start', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    }
  ]
}